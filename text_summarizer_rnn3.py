# -*- coding: utf-8 -*-
"""text_summarizer_rnn3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AQ-Af5Wy5qumSss6xciNZRXTKaoHMNV4
"""

!git clone https://github.com/sunnysai12345/News_Summary.git

path = "News_Summary/news_summary.csv"
data = pd.read_csv(path,encoding = 'ISO-8859-1')

contraction_mapping = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have", "couldn't": "could not",

                           "didn't": "did not", "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",

                           "he'd": "he would","he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",

                           "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", "I've": "I have", "i'd": "i would",

                           "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am", "i've": "i have", "isn't": "is not", "it'd": "it would",

                           "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have","it's": "it is", "let's": "let us", "ma'am": "madam",

                           "mayn't": "may not", "might've": "might have","mightn't": "might not","mightn't've": "might not have", "must've": "must have",

                           "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have","o'clock": "of the clock",

                           "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",

                           "she'd": "she would", "she'd've": "she would have", "she'll": "she will", "she'll've": "she will have", "she's": "she is",

                           "should've": "should have", "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have","so's": "so as",

                           "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is", "there'd": "there would",

                           "there'd've": "there would have", "there's": "there is", "here's": "here is","they'd": "they would", "they'd've": "they would have",

                           "they'll": "they will", "they'll've": "they will have", "they're": "they are", "they've": "they have", "to've": "to have",

                           "wasn't": "was not", "we'd": "we would", "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",

                           "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have", "what're": "what are",

                           "what's": "what is", "what've": "what have", "when's": "when is", "when've": "when have", "where'd": "where did", "where's": "where is",

                           "where've": "where have", "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have",

                           "why's": "why is", "why've": "why have", "will've": "will have", "won't": "will not", "won't've": "will not have",

                           "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have", "y'all": "you all",

                           "y'all'd": "you all would","y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have",

                           "you'd": "you would", "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",

                           "you're": "you are", "you've": "you have"}

import pandas as pd
import time
import math

import re
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

stop_words = stopwords.words('english')
def text_preprocessor(text):
  # first maping words from using contracting maping
  text = text.lower() #contvert into lower case
  text = text.split() #make tokens using on the basis of white space

  for i in range(len(text)):
    word = text[i]
    if word in contraction_mapping:
      text[i] = contraction_mapping[word]
  
  text = " ".join(text) #again convert tokens into text

  #remove stopwords from text
  text = text.split()
  temp_text = []
  for word in text:
    if word not in stop_words:
      temp_text.append(word)
  
  text = " ".join(temp_text)
  text = text.replace("'s",'') # convert your's -> your
  text = re.sub(r'\(.*\)','',text) #remove bracket inside words with brackets
  text = re.sub(r'[^a-zA-Z0-9. ]','',text) #remove punctuations
  text  = re.sub(r'\.',' . ',text)
  return text

sample = "(hello) hi there .man tiger caller who's that isn't it ? WALL-E"
text_preprocessor(sample)

data['headlines'] = data['headlines'].apply(lambda x: text_preprocessor(x))
data['text'] = data['text'].apply(lambda x: text_preprocessor(x))

x = data['text']
y = data['headlines']
MAX_LENGTH = 0
for sentence in x:
  if MAX_LENGTH < len(sentence.split()):
    MAX_LENGTH = len(sentence.split())
MAX_LENGTH

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import random
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

SOS_token = 0
EOS_token = 1

class Lang:
  def __init__(self,lang_name):
    self.lang_name = lang_name
    self.idx2word = {0:'SOS',1:'EOS'}
    self.word2idx = {}
    self.n_words = 2
    self.word2count = {}
  
  def addSentence(self,sentence):
    for word in sentence.split():
      self.addWord(word)
  def addWord(self,word):
    if word not in self.word2idx:
      self.word2idx[word] = self.n_words
      self.idx2word[self.n_words] = word
      self.word2count[word] = 1
      self.n_words +=1
    else:
      self.word2count[word] +=1

def indexesFromSentence(lang,sent):
  indexes_list = []
  for word in sent.split():
    if lang.word2idx.get(word):
      indexes_list.append(lang.word2idx[word])
  return indexes_list
  # return [lang.word2idx[word] if lang.word2idx.get(word) else UNK_token for word in sent.split()]

def tensorFromSentence(lang,sent,device=device):
  indexes = indexesFromSentence(lang,sent)
  indexes.append(EOS_token)
  return torch.tensor(indexes,dtype=torch.long,device=device).view(-1,1)

def tensorsFromPair(input_lang,output_lang,pair,device=device):
  input_tensor = tensorFromSentence(input_lang,pair[0],device)
  
  target_tensor = tensorFromSentence(output_lang,pair[1],device)
  return (input_tensor,target_tensor)

def readLangs(text,summary):
  pairs = [(text[i],summary[i]) for i in range(len(text))]
  input_lang = Lang('text')
  output_lang = Lang('summary')
  return input_lang,output_lang,pairs

def prepareDataset(text,summary):
  input_lang,output_lang,pairs = readLangs(text,summary)
  for pair in pairs:
    input_lang.addSentence(pair[0])
    output_lang.addSentence(pair[1])
  
  return input_lang,output_lang,pairs

input_lang,output_lang,pairs = prepareDataset(x,y)

class LSTMEncoder(nn.Module):
  def __init__(self,vocab_size,hidden_size,num_layers=1):
    super(LSTMEncoder,self).__init__()
    self.vocab_size = vocab_size
    self.hidden_size = hidden_size
    
    self.embeddings = nn.Embedding(num_embeddings=vocab_size,embedding_dim=hidden_size)
    
    self.lstm = nn.LSTM(hidden_size,hidden_size,num_layers=num_layers)
  
  def forward(self,X,hidden):
    
    input = X.view(1,-1)
  

    embedded = self.embeddings(input)
    
    
    output,hidden = self.lstm(embedded,hidden)
    return output,hidden
  
  def init_lstm_state(self,device):
    return (
        torch.zeros(size=(1,1,self.hidden_size),dtype=torch.float32,device=device),
        torch.zeros(size=(1,1,self.hidden_size),dtype=torch.float32,device=device)
    )

class AttnLSTMDecoder(nn.Module):
  def __init__(self,hidden_size,output_size,dropout=0.5):
    super(AttnLSTMDecoder,self).__init__()
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.dropout = dropout

    # calculating queries --> queries = W_q*Q 
    self.fc_hidden = nn.Linear(in_features=hidden_size,out_features=hidden_size,bias=False)

    #calculating keys --> keys = W_k* k
    self.fc_encoder = nn.Linear(in_features=hidden_size,out_features=hidden_size,bias=False)

    # calculating weight paramters 
    self.weight = nn.Parameter(torch.FloatTensor(1,hidden_size))

    

    self.embedding = nn.Embedding(num_embeddings=output_size,embedding_dim=hidden_size)
    self.attn_combine = nn.Linear(in_features=hidden_size*2,out_features=self.hidden_size)
    self.lstm_layer = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size)
    self.fc_classifier = nn.Linear(in_features=hidden_size,out_features=output_size)
    self.dropout = nn.Dropout(dropout)

  
  def forward(self,X,hidden,encoder_outputs):
    encoder_outptus = encoder_outputs.squeeze()
    
    X = X.view(1,-1)
    embedded = self.dropout(self.embedding(X)).view(1,-1)
    
    # calculating alignment scores
    #query is previous hidden_state(if decoder initial state previous hidden state is encoder final hidden state) and keys is encoder outputs
    x = torch.tanh(self.fc_hidden(hidden[0])+self.fc_encoder(encoder_outputs))
    
    alignment_scores = x.bmm(self.weight.unsqueeze(2))
    
    # using softmax to alignment scores to get attention weights
    attn_weights = F.softmax(alignment_scores.view(1,-1),dim=1)
    
    # multiplying the attention weights with encoder outputs to get context vector
    context_vector = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))

    # concatenating context vector with embedded input word
    attn_applied = torch.cat((embedded,context_vector[0]),1)
    decoder_input = self.attn_combine(attn_applied).unsqueeze(0)

    out,hidden = self.lstm_layer(decoder_input,hidden)
    output = F.log_softmax(self.fc_classifier(out[0]),dim=1)
    # print(output.shape)
    return output,hidden,attn_weights

def getTime(start_time):
  now_time = time.time()
  diff = now_time - start_time

  # converted first in minutes
  minutes = math.floor(diff/60)
  remaining_second = diff-minutes*60

  return f"{minutes} minutes {remaining_second:.8f} seconds"

def seq2seqTrain(encoder,decoder,input_tensor,output_tensor,encoder_optimizer,decoder_optimizer,criterion,device,max_len=MAX_LENGTH):
  input_len = len(input_tensor)
  output_len = len(output_tensor)

  encoder_optimizer.zero_grad()
  decoder_optimizer.zero_grad()

  encoder_hidden = encoder.init_lstm_state(device=device)
  encoder_outputs = torch.zeros((max_len+1,encoder.hidden_size),device=device)
  for ei in range(input_len):
    encoder_input = input_tensor[ei]
    encoder_output,encoder_hidden = encoder(encoder_input,encoder_hidden)
    encoder_outputs[ei] = encoder_output
  
  decoder_hidden = encoder_hidden
  decoder_input = torch.tensor([[SOS_token]],dtype=torch.long,device=device)
  # decoder(decoder_input,decoder_hidden,encoder_outputs)
  loss = 0.0
  teacher_force_ratio = 0.5
  use_teacher_force = True if random.random() < teacher_force_ratio else False
  
  if use_teacher_force:
    for di in range(output_len):
      decoder_output,decoder_hidden,decoder_attn = decoder(decoder_input,decoder_hidden,encoder_outputs)
      loss += criterion(decoder_output,output_tensor[di])

      decoder_input = output_tensor[di]
  else:
    for di in range(output_len):
      decoder_output,decoder_hidden,decoder_attn = decoder(decoder_input,decoder_hidden,encoder_outputs)
      loss += criterion(decoder_output,output_tensor[di])
      topv,topi = decoder_output.topk(1)
      decoder_input = topi.squeeze().detach()
      if decoder_input.item() == EOS_token:
        break
      

  loss.backward()
  encoder_optimizer.step()
  decoder_optimizer.step()
  # print(loss)

  return loss.item()/output_len

def train(encoder,decoder,input_lang,output_lang,pairs,device,lr=0.01,epochs=5000,print_loss=500,plot_loss=100):
  encoder_optimizer = torch.optim.SGD(params=encoder.parameters(),lr=lr)
  decoder_optimizer = torch.optim.SGD(params=decoder.parameters(),lr=lr)
  criterion = nn.NLLLoss()
  total_loss = []
  
  plot_loss_total = 0.0
  print_loss_total = 0.0
  

  random_pairs = [random.choice(pairs) for _ in range(epochs)]

  start_time = time.time()

  for epoch in range(1,epochs+1):
    pair = random_pairs[epoch-1]
    input_tensor,output_tensor = tensorsFromPair(input_lang,output_lang,pair)
    loss = seq2seqTrain(encoder,decoder,input_tensor,output_tensor,encoder_optimizer,decoder_optimizer,criterion,device)
    print_loss_total += loss
    plot_loss_total += loss

    
    if epoch % print_loss == 0:
      print_avg_loss = print_loss_total/print_loss
      print(f" epoch {epoch} has loss {(print_avg_loss):.8f} and time take {getTime(start_time)}",end="\n\n")
      # print(f"total time take {getTime(start_time)}",end="\n\n")

      print_loss_total = 0.0
      
    
    if epoch % plot_loss == 0:
      plot_avg_loss = plot_loss_total / plot_loss
      total_loss.append(plot_avg_loss)
      plot_loss_total = 0.0

    
    
  
  return total_loss

import matplotlib.pyplot as plt
import numpy as np
def showPlot(data):
  x = np.arange(1,len(data))*100

def seq2seqEvaluate(encoder,decoder,sentence,lang,device,max_length=MAX_LENGTH):

  input_tensor = tensorFromSentence(lang,sentence,device=device)

  encoder_len = len(input_tensor)
  encoder_hidden = encoder.init_lstm_state(device=device)
  encoder_outputs = torch.zeros((max_length+1,encoder.hidden_size),device=device)
  for ei in range(encoder_len):
    encoder_input = input_tensor[ei]
    encoder_output,encoder_hidden = encoder(encoder_input,encoder_hidden)
    encoder_outputs[ei] = encoder_output
  
  decoder_hidden = encoder_hidden
  decoder_input = torch.tensor([[SOS_token]],dtype=torch.long,device=device)

  decoder_outputs = []
  for di in range(max_length):
    decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,encoder_outputs)
    topv,topi = decoder_output.topk(1)
    

    if topi.item() == EOS_token:
      decoder_outputs.append(EOS_token)
      break
    else:
      decoder_outputs.append(topi.item())

    decoder_input = topi.squeeze().detach()
  
  return decoder_outputs

enc_vocab_size = input_lang.n_words
dec_vocab_size = output_lang.n_words
hidden_size = 256
encoder = LSTMEncoder(vocab_size=enc_vocab_size,hidden_size=hidden_size).to(device)
decoder = AttnLSTMDecoder(hidden_size=hidden_size,output_size=dec_vocab_size).to(device)
# print(list(encoder.parameters()))
# print(list(decoder.parameters()))

total_losses = train(encoder,decoder,input_lang,output_lang,pairs,device,epochs=150000,print_loss=5000)

total_losses1 = train(encoder,decoder,input_lang,output_lang,pairs,device,epochs=50000,print_loss=5000)

total_losses2 = train(encoder,decoder,input_lang,output_lang,pairs,device,epochs=50000,print_loss=5000)

from google.colab import drive
drive.mount('/content/gdrive')

encoder_path = "/content/gdrive/MyDrive/deeplearning/summary_encoder1.pth"
decoder_path = "/content/gdrive/MyDrive/deeplearning/summary_decoder1.pth"

torch.save(encoder.state_dict(), encoder_path)
torch.save(decoder.state_dict(), decoder_path)

encoder.load_state_dict(torch.load(encoder_path))
decoder.load_state_dict(torch.load(decoder_path))
encoder.eval()
decoder.eval()

def randomEvaluate(encoder,decoder,input_lang,output_lang,pairs,n_samples=5):
  random_samples = [random.choice(pairs) for _ in range(n_samples)]

  for pair in random_samples:
    sentence,summary = pair
    pred_outputs = seq2seqEvaluate(encoder,decoder,sentence,input_lang,device)
    
    # print text 
    print(sentence)
    # print original summary
    print(f"-->{summary}")
    # print predicted summary 
    print(f'''<< {" ".join([output_lang.idx2word[id] for id in pred_outputs])}''',end="\n\n")

# outputs of model before training 
randomEvaluate(encoder,decoder,input_lang,output_lang,pairs)

#@title Default title text
test_dat = " 

Good News Agency carries positive and constructive news from all over the world relating to voluntary work, the work of the United Nations, non-governmental organizations and institutions engaged in improving the quality of life – news that doesn’t “burn out” in the space of a day. It is distributed free of charge through Internet to media and editorial journalists, NGOs, service associations and high schools and colleges around the world."

pair = random.choice(pairs)
text = pair[0]
summary = pair[1]
output_indexes = seq2seqEvaluate(encoder,decoder,text,input_lang,device)
outputs_words = [output_lang.idx2word[idx] for idx in output_indexes]
print(text)
print(f"original summary ==> {summary}")
print(" ".join(outputs_words))